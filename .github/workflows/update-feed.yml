name: Update RSS Feed

on:
  schedule:
    # Run every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch:  # Allow manual trigger
  push:
    branches:
      - main

# Prevent multiple runs from executing simultaneously
# If a new run is triggered while one is in progress, it will wait
concurrency:
  group: update-feed
  cancel-in-progress: false

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  # Job to read school list from newsletters.json
  setup:
    runs-on: ubuntu-latest
    outputs:
      schools: ${{ steps.set-matrix.outputs.schools }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set matrix from newsletters.json
        id: set-matrix
        run: |
          SCHOOLS=$(jq -c '[.[].slug]' newsletters.json)
          echo "schools=$SCHOOLS" >> $GITHUB_OUTPUT
          echo "Schools to process: $SCHOOLS"

  # Ensures that the image is built prior to running the matrix so that the build step is not duplicated
  build-image:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Docker Image
        uses: ./.github/actions/setup-docker-image

  # Job to generate feeds for each school in parallel
  generate-feed:
    runs-on: ubuntu-latest
    needs: [setup, build-image]
    strategy:
      matrix:
        school: ${{ fromJson(needs.setup.outputs.schools) }}
      fail-fast: false  # Continue other schools even if one fails

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # This should always hit the cache from build-image, but reusing this action simplifies logic for no cost
      - name: Setup Docker image
        uses: ./.github/actions/setup-docker-image

      - name: Restore newsletters cache for ${{ matrix.school }}
        uses: actions/cache/restore@v4
        with:
          path: cache/newsletters/
          key: ${{ runner.os }}-newsletters-${{ matrix.school }}-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-newsletters-${{ matrix.school }}-

      - name: Restore summaries cache for ${{ matrix.school }}
        uses: actions/cache/restore@v4
        with:
          path: cache/summaries/
          key: ${{ runner.os }}-summaries-${{ matrix.school }}-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-summaries-${{ matrix.school }}-

      - name: Restore parsed cache for ${{ matrix.school }}
        uses: actions/cache/restore@v4
        with:
          path: cache/parsed/
          key: ${{ runner.os }}-parsed-${{ matrix.school }}-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-parsed-${{ matrix.school }}-

      - name: Generate feed for ${{ matrix.school }}
        run: |
          docker run --rm \
            -v "$(pwd)":/app \
            -e ANTHROPIC_API_KEY="${{ secrets.ANTHROPIC_API_KEY }}" \
            talawanda-enews:latest \
            python -m newsletter_feed.main --school ${{ matrix.school }}

      - name: Save newsletters cache for ${{ matrix.school }}
        if: always()
        uses: actions/cache/save@v4
        with:
          path: cache/newsletters/
          key: ${{ runner.os }}-newsletters-${{ matrix.school }}-${{ github.run_id }}

      - name: Save summaries cache for ${{ matrix.school }}
        if: always()
        uses: actions/cache/save@v4
        with:
          path: cache/summaries/
          key: ${{ runner.os }}-summaries-${{ matrix.school }}-${{ github.run_id }}

      - name: Save parsed cache for ${{ matrix.school }}
        if: always()
        uses: actions/cache/save@v4
        with:
          path: cache/parsed/
          key: ${{ runner.os }}-parsed-${{ matrix.school }}-${{ github.run_id }}

      - name: Upload feed artifact for ${{ matrix.school }}
        uses: actions/upload-artifact@v4
        with:
          name: feed-${{ matrix.school }}
          path: |
            output/${{ matrix.school }}-feed.rss
            output/${{ matrix.school }}-items.json

  # Job to combine all feeds and deploy to GitHub Pages
  deploy:
    runs-on: ubuntu-latest
    needs: generate-feed
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all feed artifacts
        uses: actions/download-artifact@v4
        with:
          path: output-artifacts

      - name: Combine feeds into output directory
        run: |
          mkdir -p output
          # Copy newsletters.json
          cp newsletters.json output/
          # Copy all feed files from artifacts
          find output-artifacts -name "*.rss" -exec cp {} output/ \;
          find output-artifacts -name "*.json" -exec cp {} output/ \;
          ls -la output/

      - name: Generate index.html
        run: |
          python -m newsletter_feed.generate_index
          ls -la output/

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./output
          keep_files: false
